'''
The actual translator file. This will need to be refactored later.
This file takes an AST generated by Typescript.js (loaded from an 
existing file, does not call typescript or astgen.js yet), runs the flow
documented below, and dumps a resulting csv to the specified location.
Note that we request you call your files *.torc files so anyone who uses
your files clearly knows what the CSV represents and what the columns are.

Flow (translate):
1. Generate an AST from a provided json
2. Add basic metadata (max depth for the current subtree) to each node
3. Get a list of aliases for each variable
4. Walk through the AST and deobfuscate **variables only**
5. Walk through the AST and emit basic info behind the intent of the line.
        Intent = is the line used as a function or as if it were an object?
        This data is inferred. 
6. Aggregate collected intent data by dealiased name
7. Walk through the AST w/ the new metadata and emit data when we see certain
        things of interest. Reference pre-processed metadata where appropriate.
8. Translate our sequences to a pandas dataframe
9. Remove T/F and dump our dataframe

columns, missing data set to 0. All data between [0,1]:
(Any feature with a numeric name) -> Interesting data commonly found in js. 
                                     Included to provide more descriptive power
                                     to the final format. No further analysis done.
ALIASES             -> The number of aliases for this variable relative to the
                        maximum number of aliases any variable has in the document
ARG                 -> Encountered an argument attribute in our AST
ARGEXP              -> Encountered an argumentExpression attribute in our AST
BLOCK               -> Encountered a block attribute in our AST
BODY                -> Encountered a body attribute in our AST
CALLED              -> The normed number of times this function was called (relative
                        to the maximum number of times any function was called)
CATCH               -> Encountered a catch
COND                -> Encountered an if statement
DATA                -> Encountered a variable treated as if it were data
DECL                -> Encountered a declaration
FINL                -> Encountered a finally statement
FREQ                -> Normed frequency of the metadata relatively to all other tracked metadata
FUNC                -> Encountered a function call
INIT                -> Encountered a likely initialization
INTENT_DATA         -> % of time variable treated as an object
INTENT_DATA_MAX     -> amount time variable treated as an object relative to the 
                        maximum amount of time any variable was spent as an object
INTENT_FUNC         -> % of time variable treated as a function
INTENT_FUNC_MAX     -> amount time variable treated as a function relative to the 
                        maximum amount of time any variable was spent as a function
ISNUMBER            -> If text, is it a number?
KIND                -> Encountered a kind attribute in our AST
LEFT                -> Moving to the left side of an expression
LOGLEN              -> If text or a name, the log (base 768) of the text length
PARAM               -> Encountered a parameters attribute in the AST
PERCALPHA           -> The percentage of a string which is alpha characters
PERCALPHANUMERIC    -> Perc. of string alphanumeric
PERCNUM             -> Perc. of string numeric
PERCWHITESPACE      -> Perc. of string space
RIGHT               -> Entering the right side of an expression
RTRN                -> Encountered a return statement
SHANNON             -> Shannon entropy of text or label
SPECCHAR            -> Perc. of string a special character
SQBRKT              -> Encountered a square bracket
TEXT                -> Encountered text
THEN                -> Encountered a then statement
TRY                 -> Encountered a try
TXTLEN              -> Length of text relative to the longest string in the document
UNKNOWN             -> Unknown kind
Unnamed: 0          -> Bug. Doesn't appear to have a negative influence so I'm not
                        fixing this quite yet.
VARDECL             -> Encountered a variableDeclaration attribute. Note that
                        the previous declaration feature is for inferred
                        declarations.
'''
import json, time, math, re, argparse
from copy import deepcopy
from collections import defaultdict, Counter
import pandas as pd
from nodemetadata import NodeMetadata

class Translator:
    
    '''
    Assisting functions.
    _is_alias -> Is this an assignment? That is, is it some form of x = y?
    _extra_alias_names -> If an assignment, extract the alias for the variable
    _get_assignment_type -> determine the strategy required to extract the object's
                alias
    _get_name_based_on_kind -> extract the expected name based on the assignment type
    _format_denoised_name -> If a valid assignment, emit a tuple with certain metadata.
                The name for this function is from a previous version and needs
                to be changed in an upcoming version.
    ''' 
    def _is_alias(self, node):
        return 'kind' in node and node['kind'] == 202 and 'left' in node \
                and 'right' in node and 'operatorToken' in node \
                and 'kind' in node['operatorToken'] \
                and node['operatorToken']['kind'] == 58 \
                and ('name' in node['left'] or \
                     'escapedText' in node['left']) \
                and ('name' in node['right'] or \
                     'escapedText' in node['right'])
                
    def _extract_alias_names(self, node):
    
        trgt = node
        _left = trgt['left']
        _right = trgt['right']
        left_text = ''
        right_text = ''
        if 'escapedText' in _left: left_text = _left['escapedText']
        elif 'name' in _left and 'escapedText' in _left['name']: left_text = _left['name']['escapedText']
        if 'escapedText' in _right: right_text = _right['escapedText']
        elif 'name' in _right and 'escapedText' in _right['name']: right_text = _right['name']['escapedText']
        #print("Returning " + str((left_text, right_text)))
        return left_text, right_text
                
    def _get_assignment_type(self, node):
        '''
        Type 1: `function foo(bar){}`
        Assign a function
        Root: 237
        has a name
        has parameters
        has a body
        '''
        
        if 'kind' in node and node['kind'] == 237 and 'parameters' in node and \
            'body' in node and 'name' in node:
            return 1
        
        '''
        Type 2: `foo = function(bar){}`
        Assign a function
        In declarationList -> declaration:
            kind: 235
            has a name
            has an initializer
                Kind: 194
                Has parameters
                Has body
        '''
        if 'kind' in node and node['kind'] in [235, 273] and 'name' in node and \
            'initializer' in node and 'kind' in node['initializer'] and \
            node['initializer']['kind'] in [194, 189, 203]:
            return 2
        
        '''
        Type 3: `foo = 'text'` 
        Assign a pre-existing object
        Kind: 202
        Has left
            left:
                has name
            right:
                has name w/ escapedText
        Has operatorToken
            kind: 58
        '''
        
        if 'kind' in node and node['kind'] == 202 and 'left' in node and \
            'name' in node['left'] and 'right' in node and 'name' in node['right'] \
            and 'escapedText' in node['right']['name'] and 'operatorToken' in node \
            and node['operatorToken']['kind'] == 58:
            return 3
        
        '''
        Type 4: `foo = 'text'` 
        Assign a text
        Kind: 202
        Has left
            left:
                has name
            right:
                has name w/ escapedText
        Has operatorToken
            kind: 58
        '''
        
        if 'kind' in node and node['kind'] == 202 and 'left' in node and \
            'escapedText' in node['left'] and \
            'right' in node and 'text' in node['right'] and \
            'operatorToken' in node and node['operatorToken']['kind'] == 58:
            return 4
        
        '''
        Type 5: `foo = function(){}` (other type) 
        Assign a function
        TODO: Define
        '''
        
        if 'kind' in node and node['kind'] == 202 and 'left' in node and \
            'name' in node['left'] and 'right' in node and 'arguments' in node['right'] \
            and 'operatorToken' in node and node['operatorToken']['kind'] == 58:
            return 5
        
        '''
        Type 6: `foo()` (other type) 
        Runs as a function
        Kind: 189
            has expression, arguments
        '''
        
        if 'kind' in node and node['kind'] == 189 and 'expression' in node \
            and 'arguments' in node and 'name' in node['expression']:
            return 6
        
        '''
        Type 7: `foo()` (other type) 
        Runs as a function (alt)
        Kind: 189
            has expression, arguments
        '''
        
        if 'kind' in node and node['kind'] == 189 and 'expression' in node \
            and 'arguments' in node and 'escapedText' in node['expression']:
            return 7
        
        return 0

    def _get_name_based_on_kind(self, node, kind):
        try:
            if kind == 1 or kind == 2:
                #Stored under name -> escapedText
                if 'escapedText' in node['name']:
                    return node['name']['escapedText']
                else:
                    return node['name']['text']
            elif kind == 4:
                #Stored under name -> escapedText
                return node['left']['escapedText']
            elif kind == 3 or kind == 5:
                #Stored under left -> name -> escapedText
                return node['left']['name']['escapedText']
            elif kind == 6:
                #Stored under left -> name -> escapedText
                return node['expression']['name']['escapedText']
            elif kind == 7:
                #Stored under left -> name -> escapedText
                return node['expression']['escapedText']
            
            return None
        except KeyError:
            print(node, kind)
            raise Exception()
       
    def _format_denoised_name(self, vocab, v, aliases):
        try:
            v = aliases[v]
            
            part_intent_data = vocab[v].get_standardized_intent_data()
            part_intent_func = vocab[v].get_standardized_intent_func()
            part_called = vocab[v].get_standardized_called()
            part_aliases = vocab[v].get_standardized_aliases()
            part_intent_data_max = vocab[v].get_standardized_intent_data_max()
            part_intent_func_max = vocab[v].get_standardized_intent_func_max()
            
            return ('FUNC', 
                             part_intent_data, 
                             part_intent_func, 
                             part_called, 
                             part_aliases, 
                             part_intent_data_max,
                             part_intent_func_max)
            
        except KeyError as e:
            return ('UNKNOWN')
   
    '''
    "Defining" functions and otherwise
    (the misc functions)
    These are mostly about describing text strings + a read_data function
    which honestly should be spun off into a dedicated file read/write class.
    
    read_data -> Read in our AST file
    _getloglen -> return the log of a string's len (base 768, see function, clamped to 1)
    _getshannon -> return the shannon entropy for a given string, normed to [0, 1], clamped to 1
    _getnumberstatus -> return true/false if the string is a number
    '''
    def read_data(self):
        data = ''
        
        with open(self._FLOC, 'r') as f:
            data = f.read()
        
        return json.loads(data)
    
    def _getloglen(self, t):
        #768 found through trial and error
        #512 + 256 = 768, quasi-arbitrary and chosen
        #partially as an homage to my CS background
        #anything in the 600~800s should suffice
        tlen = math.log(len(t) + 1, 768)
        return min(tlen, 1.)
    
    def _getshannon(self, t):
        lent = float(len(t))
        ps = defaultdict(int)
        for x in t:
            ps[x] += 1
        ps2 = defaultdict(float)
        for x in ps:
            ps2[x] = ps[x] / lent
        v = []
        for x in ps2:
            v.append(ps2[x] * math.log(ps2[x], 2))
        return min(abs(sum(v))/8., 1.)

    def _getnumberstatus(self, t):
        try:
            float(t)
            return True
        except:
            return False
   
    '''
    Workhorse functions
    Everything here is depth-first search with extra bells and whistles.
    
    _emit_sequential_flat_single -> Translates collected metadata into our final form,
            emitting data as our traversal sees it
    _build_structural_metadata_single -> Collect document-wide metadata
            about the frequency of certain variables as we've defined thus far
    _sub_uniques_single -> Sets the text to a descriptive tuple.
    _strip_aliases_single -> Sets variable names to the first variable from its appropriate cluster
    _build_variable_vocabulary_single -> Generates a list of metadata nodes, maps them to variable names
    _build_alias_list_single -> Builds a list of aliases for each variable. 
            As in, are we assigning some variable to a new variable?
    _collapse_names -> turns "x"."y" into "x.y".
    _fuzz -> Adds the maximum depth for the current subtree to the current node
    ''' 
    def _emit_sequential_flat_single(self, node, structural_metadata, max_char_length, seq=None):
        if not seq: seq = []
        for n in node:
            tknFreq_all = structural_metadata['all']
            if n == 'escapedText' and 'escapedText_old' in node and (node[n] == 'UNKNOWN' or (type(node[n] == tuple and node[n][0] == 'UNKNOWN') )):
                tknFreq = structural_metadata['UNKNOWN']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                #print(node)
                #print(self._getshannon(node['escapedText_old']))
                #print(len(node['escapedText_old']))
                seq.append(('UNKNOWN', freq, self._getshannon(node['escapedText_old']), self._getloglen(node['escapedText_old'])))
            elif n == 'escapedText' in node and (node[n] == 'UNKNOWN' or (type(node[n] == tuple and node[n][0] == 'UNKNOWN') )):
                tknFreq = structural_metadata['UNKNOWN']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                #print(node)
                seq.append(('UNKNOWN', freq, 0.0, 0.0))
            elif n == 'kind' and node[n] in [188]: #and maybe 216
                #print("Found SQBRKT")
                tknFreq = structural_metadata['SQBRKT']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('SQBRKT', freq))
            elif n == 'escapedText':
                if node[n][0] == 'FUNC':
                    tknFreq = structural_metadata['FUNC']
                else:
                    tknFreq = structural_metadata['DATA']
                shannon = 0.0
                loglen = 0.0
                if 'escapedText_old' in node:
                    shannon = self._getshannon(node['escapedText_old'])
                    loglen = self._getloglen(node['escapedText_old'])
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append((node[n], freq, shannon, loglen))
            elif n == 'operatorToken':
                tknFreq = structural_metadata[node[n]['kind']]
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append((str(node[n]['kind']), freq))
            elif n == 'initializer':
                tknFreq = structural_metadata['INIT']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('INIT', freq))
            elif n == 'condition':
                tknFreq = structural_metadata['COND']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('COND', freq))
            elif n == 'thenStatement':
                tknFreq = structural_metadata['THEN']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('THEN', freq))
            elif n == 'body':
                tknFreq = structural_metadata['BODY']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('BODY', freq))
            elif n == 'parameters':
                tknFreq = structural_metadata['PARAM']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('PARAM', freq))
            elif n == 'declarations':
                tknFreq = structural_metadata['DECL']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('DECL', freq))
            elif n == 'finallyBlock':
                tknFreq = structural_metadata['FINL']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('FINL', freq))
            elif n == 'tryBlock':
                tknFreq = structural_metadata['TRY']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('TRY', freq))
            elif n == 'argument':
                tknFreq = structural_metadata['ARG']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('ARG', freq))
            elif n == 'catchClause':
                tknFreq = structural_metadata['CATCH']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('CATCH', freq))
            elif n == 'variableDeclaration':
                tknFreq = structural_metadata['VARDECL']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('VARDECL', freq))
            elif n == 'block':
                tknFreq = structural_metadata['BLOCK']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('BLOCK', freq))
            elif n == 'left':
                tknFreq = structural_metadata['LEFT']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('LEFT', freq))
            elif n == 'right':
                tknFreq = structural_metadata['RIGHT']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('RIGHT', freq))
            elif n == 'argumentExpression':
                tknFreq = structural_metadata['ARGEXP']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('ARGEXP', freq))
            elif n == 'text':
                #print(node[n])
                tknFreq = structural_metadata['TEXT']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                shannon = self._getshannon(node[n])
                isnumber = self._getnumberstatus(node[n])
                shannon = shannon if not isnumber else 0.0
                count_length = len(node[n])
                count_special_chars = len(re.sub('[\w\ ]+', '', node[n]))
                count_length = float(count_length) + 1.
                percwhitespace = len(re.sub('[^\ ]+', '', node[n]))
                percnum = len(re.sub('[0-9]+', '', node[n]))
                percalpha = len(re.sub('[a-zA-Z]+', '', node[n]))
                percalphanumeric = len(re.sub('[a-zA-Z0-9]+', '', node[n]))
                loglen = self._getloglen(node[n])
                seq.append(('TEXT', 
                    freq, 
                    shannon, 
                    isnumber, 
                    count_special_chars/count_length, 
                    count_length/(max_char_length + 1.),
                    percwhitespace/count_length,
                    percnum/count_length,
                    percalpha/count_length,
                    percalphanumeric/count_length,
                    loglen
                ))
            elif n == 'kind' and node[n] in [228]:
                tknFreq = structural_metadata['RTRN']
                freq = str(round( tknFreq /float(tknFreq_all + 1) , self._FUZZ_AMT))
                seq.append(('RTRN', freq))
                
            trgt = node[n]
            if type(trgt) == dict:
                seq = self._emit_sequential_flat_single(node[n], structural_metadata, max_char_length, seq)
            elif type(trgt) == list:
                for n1 in node[n]:
                    seq = self._emit_sequential_flat_single(n1, structural_metadata, max_char_length, seq)
                    
        return seq

    def _build_structural_metadata_single(self, node, max_char_length, kinds=None):
        if not kinds: kinds = []
        
        #print(node)
        for n in node:
            #print(n)
            if n == 'operatorToken':
                kinds.append(node[n]['kind'])
            elif n == 'kind' and node[n] in [188]: #and maybe 216
                kinds.append('SQBRKT')
            elif type(node[n]) == tuple and node[n][0] == 'UNKNOWN':
                kinds.append('UNKNOWN')
            elif n == 'escapedText':
                if type(node[n]) == tuple and node[n][0] == 'FUNC':
                    kinds.append('FUNC')
                elif node[n].startswith('DATA'):
                    kinds.append('DATA')
            elif n == 'initializer':
                kinds.append('INIT')
            elif n == 'condition':
                kinds.append('COND')
            elif n == 'thenStatement':
                kinds.append('THEN')
            elif n == 'body':
                kinds.append('BODY')
            elif n == 'parameters':
                kinds.append('PARAM')
            elif n == 'declarations':
                kinds.append('DECL')
            elif n == 'finallyBlock':
                kinds.append('FINL')
            elif n == 'tryBlock':
                kinds.append('TRY')
            elif n == 'argument':
                kinds.append('ARG')
            elif n == 'catchClause':
                kinds.append('CATCH')
            elif n == 'variableDeclaration':
                kinds.append('VARDECL')
            elif n == 'block':
                kinds.append('BLOCK')
            elif n == 'left':
                kinds.append('LEFT')
            elif n == 'right':
                kinds.append('RIGHT')
            elif n == 'kind' and node[n] in [188]:
                kinds.append('SQBRKT')
            elif n == 'argumentExpression':
                kinds.append('ARGEXP')
            elif n == 'text':
                kinds.append('TEXT')
                if len(node[n]) > max_char_length:
                    max_char_length = len(node[n])
            elif n == 'kind' and node[n] in [228]:
                kinds.append('RTRN')
            trgt = node[n]
            if type(trgt) == dict:
                kinds, max_char_length = self._build_structural_metadata_single(node[n], max_char_length, kinds)
            elif type(trgt) == list:
                for n1 in node[n]:
                    kinds, max_char_length = self._build_structural_metadata_single(n1, max_char_length, kinds)
                    
        return kinds, max_char_length

    def _sub_uniques_single(self, node, vocab, aliases):
        for n in node:
            if n == 'escapedText':
                newname = self._format_denoised_name(vocab, node[n], aliases)
                node['escapedText'] = newname
            trgt = node[n]
            if type(trgt) == dict:
                self._sub_uniques_single(node[n], vocab, aliases)
            elif type(trgt) == list:
                for n1 in node[n]:
                    self._sub_uniques_single(n1, vocab, aliases)
                    
    def _strip_aliases_single(self, node, aliases):
        for n in node:
            if type(node[n]) == dict:
                self._strip_aliases_single(node[n], aliases)
            elif type(node[n]) == list:
                for n1 in node[n]:
                    self._strip_aliases_single(n1, aliases)
            elif n == 'escapedText':
                for a in aliases:
                    if node[n] in a:
                        node['escapedText'] = a[0]
            elif n == 'text':
    
                for a in aliases:
                    if node[n] in a:
    
                        node['text'] = a[0]

    def _build_variable_vocabulary_single(self, node, vocab=None, missing_names=None):
        if not vocab:
            vocab = defaultdict(list)
        if not missing_names:
            missing_names = []
            
        #Assign a variable
        #determine strategy for grabbing name
        kind = self._get_assignment_type(node)
            
        if kind:
            #apply strategy to grab name
            left_name = self._get_name_based_on_kind(node, kind)
            t_node = NodeMetadata()
            t_node.set_name(left_name)
            t_node.set_intent(kind)
            
            #why do we even append again? is this for frequencies later?
            vocab[left_name].append(t_node)
        elif 'escapedText' in node and not (node['escapedText'] in vocab):
            #dropped some stuff about k
            missing_names.append(node['escapedText'])
            
        for n in node:
            trgt = node[n]
            if type(trgt) == dict:
                vocab, missing_names = self._build_variable_vocabulary_single(node[n], vocab, missing_names)
            elif type(trgt) == list:
                for n1 in node[n]:
                    vocab, missing_names = self._build_variable_vocabulary_single(n1, vocab, missing_names)
                    
        return vocab, missing_names
            
    def _build_alias_list_single(self, node, aliases=None):
        if not aliases:
            aliases = defaultdict(str)
        
        #We define an alias as expression: left (depth<=1), op (kind=58), right (depth<=1)
        node_is_alias = False
        if self.is_alias(node):
            #We have an alias!
            left_name, right_name = self._extract_alias_names(node)
            if left_name and right_name:
                node_is_alias = True
                aliases[left_name] = right_name
                aliases[right_name] = left_name
        if not node_is_alias:
            for n in node:
                trgt = node[n]
                if type(trgt) == dict:
                    aliases = self._build_alias_list_single(node[n], aliases)
                elif type(trgt) == list:
                    for n1 in node[n]:
                        aliases = self._build_alias_list_single(n1, aliases)
        
        return aliases

    def _collapse_names(self, node, _search=False):
        cur_depth = node['maxdepth']
        ignore_types = ['pos', 'end', 'flags', 'kind', 'maxdepth', 'modifierFlagsCache']
        valid_subtypes = ['expression', 'name']
        
        if 'escapedText' in node: node['escapedText_old'] = node['escapedText']
    
        if (not 'name' in node) or (not 'expression' in node) or ('escapedText' not in node['name'])\
         or ('escapedText' not in node['expression']):
            #Definitely not a name
            for n in node:
                trgt = node[n]
                if type(trgt) == dict:
                    self._collapse_names(node[n])
                elif type(trgt) == list:
                    for n1 in node[n]:
                        self._collapse_names(n1)
                        
        elif 'name' in node and 'expression' in node and cur_depth == 1:
            #In a leaf node
            tname = node['expression']['escapedText'] + '.' + node['name']['escapedText']
            node['name']['escapedText'] = tname
            return tname
        elif 'expression' in node and 'name' in node:
            partialname = self._collapse_names(node['expression'], _search=True)
            if not partialname: return
            if not _search:
                node['name']['escapedText'] = partialname + '.' + node['name']['escapedText']
            else:
                return partialname + '.' + node['name']['escapedText']
        else:
            pass
        
    def _fuzz(self, node):
        maxdepth = 0
        for n in node:
            trgt = node[n]
            if type(trgt) == dict:
                depth = self._fuzz(node[n])
                if depth > maxdepth:
                    maxdepth = depth
            elif type(trgt) == list:
                for n1 in node[n]:
                    depth = self._fuzz(n1)
                    if depth > maxdepth:
                        maxdepth = depth
        node['maxdepth'] = maxdepth
        return maxdepth + 1
   
    '''
    Batch functions
    These call the *_single versions for each line in our AST
    
    emit_sequential_flat -> per-line operation for _emit_sequential_flat_single
    build_structural_metadata -> per-line operation for _build_structural_metadata_single
    sub_uniques -> per-line operation for _sub_uniques_single
    build_variable_vocabulary -> per-line operation for _build_variable_vocabulary_single
    build_alias_list -> per-line operation for _build_alias_list_single
    strip_aliases -> per-line operation for _strip_aliases_single
    seq_to_pandas -> Translates our stream of lines (generated by emit_sequential)
            and turns it into a pandas dataframe
    minimize_vocab -> Takes a list of metadata for each key and creates a single
            metadata node w/ the appropriate frequency data
    '''
    def emit_sequential_flat(self, statements, structural_metadata, max_char_length):
        sequences = []
        i = 0
        for s in statements:
            print((i, len(statements)))
            sequences += self._emit_sequential_flat_single(s, structural_metadata, max_char_length)
    
        return sequences

    def build_structural_metadata(self, statements, max_char_length):
        metadata = []
        
        for v in statements:
            t_metadata, t_max_char_length = self._build_structural_metadata_single(v, max_char_length)
            if t_max_char_length > max_char_length:
                max_char_length = t_max_char_length
            metadata += t_metadata
    
        metadata = Counter(metadata)
        metadata['all'] = sum([metadata[x] for x in metadata])
    
        return metadata, max_char_length

    def sub_uniques(self, statements, vocab, aliases):
        for t in statements:
            self._sub_uniques_single(t, vocab, aliases)
    
    def build_variable_vocabulary(self, statements):
        all_vocab = {}
        missing_names_all = []
        for v in statements:
            vocab, missing_names = self._build_variable_vocabulary_single(v)
            missing_names_all += missing_names
            
            for v1 in vocab:
                if v1 in all_vocab:
                    all_vocab[v1] += vocab[v1]
                else:
                    all_vocab[v1] = vocab[v1]
                    
        missing_names_all = [x for x in missing_names_all if not x in all_vocab] 
    
        return all_vocab
    
    def build_alias_list(self, statements):
        
        aliases = []
        for v in statements:
            self._fuzz(v)
            self._collapse_names(v)
            _aliases = self._build_alias_list_single(v)
            for x in _aliases:
                trgt = _aliases[x]
                found = False
                for c in aliases:
                    if trgt in c:
                        found = True
                        c.append(x)
                    elif x in c:
                        found = True
                        c.append(trgt)
                if not found:
                    aliases.append([x, trgt])
        
        aliases = [list(set(x)) for x in aliases]
        
        return aliases

    def strip_aliases(self, statements, aliases):
        for v in statements:
            self._strip_aliases_single(v, aliases)
        
    def seq_to_pandas(self, sequence):
        '''
        FUNC!
        part_intent_data, 
        part_intent_func, 
        part_called, 
        part_aliases, 
        part_intent_data_max,
        part_intent_func_max
        '''
        
        colz = ['26', '27', '29', '30', '31', '32', '33', '34', '35', '37', '38', '39', '40', '41', '42', 
                '45', '46', '47', '48', '49', '50', '53', '54', '58', '59', '60', '61', '63', '64', '65', 
                '66', '67', '68', '69', '70', '92', '93', 'ALIASES', 'ARG', 'ARGEXP', 'BLOCK', 'BODY', 
                'CALLED', 'CATCH', 'COND', 'DATA', 'DECL', 'FINL', 'FREQ', 'FUNC', 'INIT', 'INTENT_DATA', 
                'INTENT_DATA_MAX', 'INTENT_FUNC', 'INTENT_FUNC_MAX', 'ISNUMBER', 'KIND', 'LEFT', 'LOGLEN', 
                'PARAM', 'PERCALPHA', 'PERCALPHANUMERIC', 'PERCNUM', 'PERCWHITESPACE', 'RIGHT', 'RTRN', 
                'SHANNON', 'SPECCHAR', 'SQBRKT', 'TEXT', 'THEN', 'TRY', 'TXTLEN', 'UNKNOWN', 'Unnamed: 0', 
                'VARDECL']
    
        #colz2 = list(set([x[0] for x in sequence]))
        #colz = list(set(colz + colz2))
    
    
        i = 0
        dflist = []
        for s in sequence:
            if i % 5000 == 0:
                print(i, len(sequence))
            base_template = {c: 0. for c in colz}
            if type(s[0]) == tuple and s[0][0] == 'FUNC':
                parts = s[0]
                intent_data = parts[1]
                intent_func = parts[2]
                called_count = parts[3]
                alias_count = parts[4]
                intent_data_max = parts[5]
                intent_func_max = parts[6]
                freq = s[1]
                shannon = s[2]
                loglen = s[3]
                base_template['INTENT_DATA'] = intent_data
                base_template['INTENT_FUNC'] = intent_func
                base_template['CALLED'] = called_count
                base_template['ALIASES'] = alias_count
                base_template['INTENT_DATA_MAX'] = intent_data_max
                base_template['INTENT_FUNC_MAX'] = intent_func_max
                base_template['FREQ'] = freq
                base_template['SHANNON'] = shannon
                base_template['LOGLEN'] = loglen
            else:
                parts = s
                if s[0] not in colz: continue
                calltype = parts[0]
                freq = parts[1]
                base_template[calltype] = 1.
                base_template['FREQ'] = freq
                if len(s) >= 3:
                    shannon = parts[2]
                    loglen = parts[3]
                    print("Shannon: " + str(shannon))
                    base_template['SHANNON'] = shannon
                    base_template['LOGLEN'] = loglen
                try:
                    shannon = parts[2]
                    isnumber = parts[3]
                    special_char_ratio = parts[4]
                    textlen = parts[4]
                    PERCWHITESPACE = parts[5]
                    PERCNUM = parts[6]
                    PERCALPHA = parts[7]
                    PERCALPHANUMERIC = parts[8]
                    loglen = parts[9]
                    base_template['SHANNON'] = shannon
                    base_template['ISNUMBER'] = 1.0 if isnumber in ['True', True, 1.0] else 0.0
                    base_template['SPECCHAR'] = special_char_ratio
                    base_template['TXTLEN'] = textlen
                    base_template['PERCWHITESPACE'] = PERCWHITESPACE
                    base_template['PERCNUM'] = PERCNUM
                    base_template['PERCALPHA'] = PERCALPHA
                    base_template['PERCALPHANUMERIC'] = PERCALPHANUMERIC
                    base_template['LOGLEN'] = loglen
                except IndexError:
                    pass
            dflist.append(base_template)
            i += 1
        df = pd.DataFrame(dflist)
        df2 = df
        for c in colz:
            if c not in df2.columns.values:
                df2[c] = 0.
        return df2.fillna(0.)

    def minimize_vocab(self, vocab, aliases):
        new_vocab = {}
        for v in vocab:
            vlist = vocab[v]
            #magic number what is 3?
            filtered_intents = [x.get_intent() for x in vlist if x.get_intent() != 3]
            
            data_calls = 0
            func_calls = 0
            for instance in vlist:
                intent = instance.get_intent()
                if intent == 3: continue
                if intent == 4: 
                    data_calls += 1
                else: 
                    func_calls += 1
                    
                
            new_metadata = NodeMetadata()
            new_metadata.set_name(v)
            
            new_metadata.set_func_intent_count(func_calls)
            new_metadata.set_data_intent_count(data_calls)
            
            #shouldn't this be using our reference? our map ref?
            local_aliases = []
            for a in aliases:
                if v in a:
                    local_aliases = a
            new_metadata.set_aliases(list(set(local_aliases)))
            [new_metadata.called() for x in vlist]
            new_vocab[v] = new_metadata
        return new_vocab

    '''
    The magic happens below
    '''
    def translate(self):
        
        #load raw javascript
        raw_js = self.read_data()
        
        #create a list of *assumed* variable aliases
        aliases = self.build_alias_list(raw_js)
        
        #Change our aliases to a single variable for each cluster
        self.strip_aliases(raw_js, aliases)
        
        #build our vocab (add our metadata object to each node)
        '''
        Update: Create a list of instances for each variable.
        Then collect information on how the variable was used in each instance.
        Finally, use the minimize vocab function to aggregate our results
        into a single descriptive datapoint for that variable name.
        
        Do we still add a metadata object to each ndoe? I don't think we do...
        '''
        vocab = self.build_variable_vocabulary(raw_js)
        
        #create an overarching list of metadata
        #to generate document-wide metadata
        #...then update our metadata "vocab" with those document-wide data
        min_vocab = self.minimize_vocab(vocab, aliases)
        for x in min_vocab:
            min_vocab[x].set_standards(min_vocab)
        
        #map each alias to a cluster
        #speeds up computation later
        new_aliases = {}
        for clstr in aliases:
            for obj1 in clstr:
                new_aliases[obj1] = clstr[0]
            
        #sub the unique nodes with our summarized metadata
        self.sub_uniques(raw_js, min_vocab, new_aliases)
        
        #more document-wide metadata harvesting, also get our max char length
        #so we can normalize some frequencies or something. Review
        #everything starting with vocab
        #update: builds frequencies for tracked tags
        max_char_length = 0
        metadata, max_char_length = self.build_structural_metadata(raw_js, max_char_length)
        
        #walk through our metadataized tree and emit important facts as we
        #see them
        #also why do we use metadata? it'll literally be Counter().
        sequences = emit_sequential_flat(raw_js, metadata, max_char_length)
        
        #translate the facts to our pandas dataframe
        pdc = seq_to_pandas(sequences)
        pdc = pdc * 1 #remove true, false

        #dump the result
        pdc.to_csv(self._TARGET_FLOC)
    
    def __init__(self, file_loc, target_loc): 
        self._FUZZ_AMT = 10
        self._FLOC = file_loc
        self._TARGET_FLOC = target_loc
        
if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("inputfile")
    parser.add_argument("outputfile")
    args = parser.parse_args()
    outfile_torc = args.outputfile
    if not outfile_torc.endswith('torc'): outfile_torc += '.torc'
    translator = Translator(args.intputfile, outfile_torc)
    translator.translate()
